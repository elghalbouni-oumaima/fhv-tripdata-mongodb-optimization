"""
Automatic Slow Query Detection & Index Benchmarking
---------------------------------------------------

Ce script :
1. Ex√©cute 10 requ√™tes candidates
2. Mesure l'explain() BEFORE index
3. Sauvegarde les temps d'ex√©cution (execution_time.json)
4. Si une requ√™te est lente ‚Üí cr√©e un index adapt√©
5. Mesure l'explain() AFTER index
6. Sauvegarde metrics avant/apr√®s dans JSON
7. Optimis√© pour collections volumineuses

Auteur : Optimis√© par ChatGPT
"""

from src.mongo_import import connect_to_mongo
import json
from datetime import datetime
import os
from src.logger import logger

# -------------------------------------------------------------------
# CONFIG
# -------------------------------------------------------------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
RESULTS_DIR = os.path.abspath(os.path.join(BASE_DIR, "../..", "results", "benchmarking"))

DB_NAME = "trips_db"
COLLECTION_NAME = "fhvhv_trips_2021-10"

db = connect_to_mongo(DB_NAME)
collection = db[COLLECTION_NAME]


# -------------------------------------------------------------------
# 1 ‚Äî LISTE DES REQU√äTES DE TEST
# -------------------------------------------------------------------
SLOW_QUERY_CANDIDATES = [
    # ---------------------------------------------------------
    # TYPE 1: INDEX SIMPLE (Single Field)
    # Objectif: Passer d'un scan complet √† un scan cibl√© sur des valeurs rares.
    # ---------------------------------------------------------
    {
        "name": "q1_simple_outlier_range",
        # AVANT: Scanne toute la base.
        # APR√àS: Scanne seulement les quelques trajets > 5000s.
        "query": {"trip_time": {"$gte": 5000}}, 
        "index": {"trip_time": 1}
    },
    {
        "name": "q2_simple_sort_blocking",
        # AVANT: "Blocking Sort" (Erreur m√©moire possible ou tr√®s lent).
        # APR√àS: R√©sultat instantan√© car l'index est d√©j√† tri√©.
        "query": {"trip_miles": {"$gte": 5}},
        "sort": {"trip_miles": -1}, 
        "index": {"trip_miles": 1}
    },
    {
        "name": "q3_simple_distinct_lookup",
        # Chercher une valeur pr√©cise rare.
        "query": {"dispatching_base_num": "B02800"}, 
        "index": {"dispatching_base_num": 1}
    },

    # ---------------------------------------------------------
    # TYPE 2: INDEX HASHED (Hach√©)
    # Objectif: √âgalit√© stricte uniquement. Tr√®s rapide pour le point lookup.
    # ---------------------------------------------------------
    {
        "name": "q4_hashed_equality",
        # Le hachage distribue les valeurs. Id√©al pour des IDs.
        "query": {"hvfhs_license_num": "HV0003"},
        "index": {"hvfhs_license_num": "hashed"}
    },
    {
        "name": "q5_hashed_location",
        # Recherche exacte sur un ID de lieu.
        "query": {"PULocationID": 132},
        "index": {"PULocationID": "hashed"}
    },

    # ---------------------------------------------------------
    # TYPE 3: INDEX COMPOUND (Compos√©)
    # R√®gle d'or ESR : Equality (√âgalit√©) -> Sort (Tri) -> Range (Plage)
    # ---------------------------------------------------------
    {
        "name": "q6_compound_ESR_perfect",
        # Respecte la r√®gle ESR.
        # Equality: PULocationID, Sort: trip_miles, Range: trip_time
        "query": {"PULocationID": 79, "trip_time": {"$gt": 600}},
        "sort": {"trip_miles": 1},
        "index": {"PULocationID": 1, "trip_miles": 1, "trip_time": 1}
    },
    {
        "name": "q7_compound_covered_query",
        # *** TRES IMPORTANT *** : REQU√äTE COUVERTE
        # Si on ne demande QUE les champs de l'index (_id: 0), 
        # DocsExamined tombera √† 0. C'est l'optimisation ultime.
        "query": {"hvfhs_license_num": "HV0005", "trip_miles": {"$gte": 2}},
        "projection": {"hvfhs_license_num": 1, "trip_miles": 1, "_id": 0},
        "index": {"hvfhs_license_num": 1, "trip_miles": 1}
    },
    {
        "name": "q8_compound_sort_optimization",
        # Sans index: MongoDB doit trouver tous les B02510 puis les trier en RAM.
        # Avec index: Il lit l'index dans l'ordre. Il s'arr√™te d√®s qu'il a les 10 premiers.
        "query": {"dispatching_base_num": "B02510"},
        "sort": {"request_datetime": -1},
        "limit": 10,
        "index": {"dispatching_base_num": 1, "request_datetime": -1}
    },
    {
        "name": "q9_compound_multi_equality",
        # Filtrage pr√©cis sur deux champs. R√©duit drastiquement l'ensemble scann√©.
        "query": {
            "PULocationID": 138, 
            "DOLocationID": 230,
            "shared_request_flag": 1
        },
        "index": {"PULocationID": 1, "DOLocationID": 1}
    },
    {
        "name": "q10_compound_range_sort_heavy",
        # Un cas lourd : Plage de date + Tri sur miles.
        # Avant : Lent car beaucoup de donn√©es dans la plage de dates.
        # Apr√®s : L'index aide √† filtrer, mais surtout √† √©viter le tri m√©moire.
        "query": {"request_datetime": {"$gte": "2019-01-01", "$lt": "2019-02-01"}},
        "sort": {"trip_miles": -1},
        "index": {"request_datetime": 1, "trip_miles": -1}
    }
]

# -------------------------------------------------------------------
# 2 ‚Äî D√©tection automatique du type d'index
# -------------------------------------------------------------------
def detect_index_type(index_param):
    """
    D√©tecte le type d‚Äôindex :
    - simple index
    - hashed index
    - compound index
    """
    items = list(index_param.items())

    if len(items) == 1 and items[0][1] == "hashed":
        return "hashed index"

    if len(items) == 1:
        return "simple index"

    return "compound index"


# -------------------------------------------------------------------
# 3 ‚Äî EXPLAIN OPTIMIS√â
# -------------------------------------------------------------------

def run_explain(query, coll=collection):
    """
    Explain optimis√© :
    - limit(10000) pour √©viter les scans complets
    - .explain() sans param√®tre (compatibilit√© pyMongo)
    """
    cursor = coll.find(query).limit(10000)
    explain_data = cursor.explain()

    stats = explain_data.get("executionStats", {})
    planner = explain_data.get("queryPlanner", {})

    return {
        "executionTimeMillis": stats.get("executionTimeMillis"),
        "optimizationTimeMillis": planner.get("optimizationTimeMillis"),
        "totalDocsExamined": stats.get("totalDocsExamined"),
        "totalKeysExamined": stats.get("totalKeysExamined"),
        "nReturned": stats.get("nReturned"),
        "executionStages": stats.get("executionStages"),
        "indexName": (
            planner.get("winningPlan", {})
                   .get("inputStage", {})
                   .get("indexName")
        )
    }

# -------------------------------------------------------------------
# 4 ‚Äî Save BEFORE execution time for ALL queries
# -------------------------------------------------------------------
def save_execution_times():
    """
    Sauvegarde les temps BEFORE index
    dans execution_time.json
    """
    os.makedirs(RESULTS_DIR, exist_ok=True)
    results = []

    logger.info("‚è≥ Collecting execution times BEFORE indexing...")

    for q in SLOW_QUERY_CANDIDATES:
        name = q["name"]
        query = q["query"]
        index_param = q["index"]
        index_type = detect_index_type(index_param)

        explain_res = run_explain(query)
        exec_time = explain_res["executionTimeMillis"]

        results.append({
            "query_name": name,
            "query": query,
            "executionTimeMillis": exec_time,
            "index_type": index_type
        })

        logger.info(f"‚Üí {name}: {exec_time} ms ({index_type})")

    # SAVE FILE
    path = os.path.join(RESULTS_DIR, "execution_time.json")
    with open(path, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=4)

    logger.info(f"‚úî execution_time.json saved ‚Üí {path}")


# -------------------------------------------------------------------
# 5 ‚Äî SAVE METRICS BEFORE/AFTER INDEX
# -------------------------------------------------------------------
def save_metrics(name, before, after, index_param):
    """
    Sauvegarde les metrics BEFORE/AFTER dans JSON
    """
    os.makedirs(RESULTS_DIR, exist_ok=True)

    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    filename = f"{name}_{timestamp}.json"
    path = os.path.join(RESULTS_DIR, filename)

    data = {
        "query_name": name,
        "index_param": index_param,
        "index_type": detect_index_type(index_param),
        "results": {
            "before": before,
            "after": after
        }
    }

    with open(path, "w") as f:
        json.dump(data, f, indent=4)

    logger.info(f"‚úî Saved benchmark ‚Üí {path}")


# -------------------------------------------------------------------
# 6 ‚Äî DROP INDEXES intelligently
# -------------------------------------------------------------------

def drop_conflicting_indexes(index_param):
    """
    Supprime TOUS les index qui commencent par le m√™me champ 
    que l'index propos√©. Cela √©vite que MongoDB utilise un index 
    compos√© existant (ex: 'trip_time_1_miles_1') pour optimiser 
    une requ√™te sur 'trip_time'.
    """
    try:
        info = collection.index_information()
        
        # On r√©cup√®re le premier champ de l'index qu'on veut tester
        # Ex: Si index_param est {"trip_time": 1}, target_root = "trip_time"
        target_root = list(index_param.keys())[0]

        for index_name, meta in info.items():
            if index_name == "_id_":
                continue

            existing_keys = meta["key"] # Ex: [('trip_time', 1), ('trip_miles', -1)]
            existing_root = existing_keys[0][0] # Le premier champ de l'index existant

            # Si l'index existant commence par le m√™me champ, il faut le supprimer
            # sinon le benchmark "Before" sera fauss√© (IXSCAN au lieu de COLLSCAN)
            if existing_root == target_root:
                logger.warning(f"üßπ Dropping interfering index '{index_name}'...")
                collection.drop_index(index_name)

    except Exception as e:
        logger.error(f"Error checking indexes: {e}")

# -------------------------------------------------------------------
# 7 ‚Äî MAIN: Slow Query Detection
# -------------------------------------------------------------------
def run_slow_query_detection(threshold_ms=200):
    logger.info("üöÄ Starting slow query detection...")

    # Step 1 ‚Äî Save ALL BEFORE execution times
    save_execution_times()

    # Step 2 ‚Äî Process each query
    for q in SLOW_QUERY_CANDIDATES:
        name = q["name"]
        query = q["query"]
        index_param = q["index"]

        logger.info(f"\n=== TEST {name} ===")

        # 1. D'ABORD on nettoie
        drop_conflicting_indexes(index_param) 

        # 2. ENSUITE on mesure (on est s√ªr que c'est lent maintenant)
        before = run_explain(query)
        time_before = before["executionTimeMillis"]

        logger.info(f"‚è± BEFORE = {time_before} ms")

        if time_before <= threshold_ms:
            logger.info(f"‚Üí Query {name} is FAST (<{threshold_ms} ms). Skipped.")
            continue

        logger.warning(f"‚ö† SLOW QUERY ‚Üí Creating index {index_param}")

        collection.create_index(list(index_param.items()))

        after = run_explain(query)

        logger.info(f"‚è± AFTER = {after['executionTimeMillis']} ms")

        # Save BEFORE/AFTER comparison
        save_metrics(name, before, after, index_param)

    logger.info("üèÅ Slow query detection finished.")


# -------------------------------------------------------------------
# 8 ‚Äî RUN SCRIPT
# -------------------------------------------------------------------
if __name__ == "__main__":
    logger.info("===== STARTING BENCHMARK ENGINE =====")
    run_slow_query_detection(threshold_ms=200)
    logger.info("===== FINISHED =====")
